{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "document_irasuto_recommender.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj6CdHkLiElp"
      },
      "source": [
        "# あなたの文章に合った「いらすとや」画像をレコメンド♪（応用編）\n",
        "\n",
        "解説記事: https://qiita.com/sonoisa/items/27527c741ae93ddd6df4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HywTtx2I9Jli"
      },
      "source": [
        "%%time\n",
        "!pip install mecab-python3==1.0.3 ipadic==1.0.0 pymagnitude ja-sentence-segmenter gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0ui_asOzmm2"
      },
      "source": [
        "!wget http://gensen.dl.itc.u-tokyo.ac.jp/soft/pytermextract-0_01.zip\n",
        "!unzip pytermextract-0_01.zip\n",
        "!cd pytermextract-0_01; python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWGqJURBcX6i"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?export=view&id=1crJlEb5IrOwf_vf6icDF14u-DDxkE3OA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTttxMeQfe7S"
      },
      "source": [
        "!gdown \"https://drive.google.com/uc?export=view&id=1DZjgYCda82IYAUhbmsJin5A8y9Y-lNyw\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4vFZ8Pl_s87"
      },
      "source": [
        "from pymagnitude import *\n",
        "\n",
        "fasttext_model = Magnitude(\"jawiki.base_lowercase.ipadic.fasttext.ws5-neg5-epoch5.magnitude\", \n",
        "                           normalized=True, ngram_oov=True, case_insensitive=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coMTs4EJgJQX"
      },
      "source": [
        "%%time\n",
        "similarities = fasttext_model.most_similar(positive=['王子', '女'], negative=['男'])\n",
        "print(similarities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Offh5t0egMMf"
      },
      "source": [
        "import numpy as np\n",
        "def cos_sim(v1, v2):\n",
        "    v1 = v1 / np.linalg.norm(v1, axis=0, ord=2)\n",
        "    v2 = v2 / np.linalg.norm(v2, axis=0, ord=2)\n",
        "    return 1 - np.sum(v1 * v2)\n",
        "\n",
        "def euclid_sim(v1, v2):\n",
        "    return np.linalg.norm(v2 - v1, ord=2)\n",
        "\n",
        "def l1_sim(v1, v2):\n",
        "    return np.linalg.norm(v2 - v1, ord=1)\n",
        "\n",
        "def sentence_similarity(v1, v2):\n",
        "    '''文ベクトルの類似度を返す。小さいほど類似している。'''\n",
        "    return cos_sim(v1, v2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNP2Lku-gb1J"
      },
      "source": [
        "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    s = s.lower()\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUKopvBPgepg"
      },
      "source": [
        "def normalize_text(text):\n",
        "    return normalize_neologd(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNTEhOoHggjX"
      },
      "source": [
        "import MeCab\n",
        "import ipadic\n",
        "\n",
        "mecab = MeCab.Tagger(ipadic.MECAB_ARGS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfhTrgB1g9NS"
      },
      "source": [
        "class Morph(object):\n",
        "    def __init__(self, surface, pos, base, line):\n",
        "        self.surface = surface\n",
        "        self.pos = pos\n",
        "        self.base = base\n",
        "        self.line = line\n",
        "    def __repr__(self):\n",
        "        return str({\n",
        "            \"surface\": self.surface,\n",
        "            \"pos\": self.pos,\n",
        "            \"base\": self.base\n",
        "        })\n",
        "\n",
        "def tokenize(sentence):\n",
        "    sentence = normalize_text(sentence)\n",
        "    mecab.parse(\"\")\n",
        "    lines = mecab.parse(sentence).split(\"\\n\")\n",
        "    tokens = []\n",
        "    for line in lines:\n",
        "        elems = line.split(\"\\t\")\n",
        "        if len(elems) < 2:\n",
        "            continue\n",
        "        surface = elems[0]\n",
        "        if len(surface):\n",
        "            feature = elems[1].split(\",\")\n",
        "            base = surface if len(feature) < 7 or feature[6] == \"*\" else feature[6]\n",
        "            pos = \",\".join(feature[0:4])\n",
        "            tokens.append(Morph(surface=surface, pos=pos, base=base, line=line))\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhAgsAudhZZ9"
      },
      "source": [
        "tokenize(\"MeCabを用いて正規化済み文字列を形態素解析します！！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxx11g-Lhbhc"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('irasuto_items_part.json', 'r', encoding=\"utf-8\") as items_file:\n",
        "    items = json.load(items_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpGatGxohecN"
      },
      "source": [
        "stop_pos = {\n",
        "    \"助詞,格助詞,一般,*\",\n",
        "    \"助詞,格助詞,引用,*\",\n",
        "    \"助詞,格助詞,連語,*\",\n",
        "    \"助詞,係助詞,*,*\",\n",
        "    \"助詞,終助詞,*,*\",\n",
        "    \"助詞,接続助詞,*,*\",\n",
        "    \"助詞,特殊,*,*\",\n",
        "    \"助詞,副詞化,*,*\",\n",
        "    \"助詞,副助詞,*,*\",\n",
        "    \"助詞,副助詞／並立助詞／終助詞,*,*\",\n",
        "    \"助詞,並立助詞,*,*\",\n",
        "    \"助詞,連体化,*,*\",\n",
        "    \"助動詞,*,*,*\",\n",
        "    \"記号,句点,*,*\",\n",
        "    \"記号,読点,*,*\",\n",
        "    \"記号,空白,*,*\",\n",
        "    \"記号,一般,*,*\",\n",
        "    \"記号,アルファベット,*,*\",\n",
        "    \"記号,一般,*,*\",\n",
        "    \"記号,括弧開,*,*\",\n",
        "    \"記号,括弧閉,*,*\",\n",
        "    \"動詞,接尾,*,*\",\n",
        "    \"動詞,非自立,*,*\",\n",
        "    \"名詞,非自立,一般,*\",\n",
        "    \"名詞,非自立,形容動詞語幹,*\",\n",
        "    \"名詞,非自立,助動詞語幹,*\",\n",
        "    \"名詞,非自立,副詞可能,*\",\n",
        "    \"名詞,接尾,助動詞語幹,*\",\n",
        "    \"名詞,接尾,人名,*\",\n",
        "    \"接頭詞,名詞接続,*,*\"\n",
        "}\n",
        "\n",
        "vocab = {}\n",
        "for item in items:\n",
        "    desc = item[\"desc\"]\n",
        "    title = item[\"title\"]\n",
        "    tokens = tokenize(desc)\n",
        "    for token in tokens:\n",
        "        key = token.base\n",
        "        pos = token.pos\n",
        "        is_stop = pos in stop_pos\n",
        "        v = vocab.get(key, { \"count\": 0, \"pos\": pos , \"stop\": is_stop})\n",
        "        v[\"count\"] += 1\n",
        "        vocab[key] = v\n",
        "\n",
        "vocab_list = []\n",
        "for k in vocab:\n",
        "    v = vocab[k]\n",
        "    if not v[\"stop\"]:\n",
        "        vocab_list.append((v[\"count\"], k, v[\"pos\"], v[\"stop\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCsJpQnzhhIt"
      },
      "source": [
        "vocab_list = sorted(vocab_list, reverse=True)\n",
        "vocab_list[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnVZtE96hjy0"
      },
      "source": [
        "stop_word = [w[1] for w in vocab_list[:2]]\n",
        "stop_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOwrS1_ZhmXC"
      },
      "source": [
        "import re\n",
        "stop_word_regex = [ re.compile(\"^([!?]+|\\)。)$\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ma_zbpNLhpdU"
      },
      "source": [
        "def get_token_vectors(sentence):\n",
        "    tokens = tokenize(sentence)\n",
        "    vecs = []\n",
        "    for token in tokens:\n",
        "        if is_stop(token):\n",
        "            continue\n",
        "        # query = token.surface\n",
        "        query = token.base\n",
        "        v = fasttext_model.query(query)\n",
        "        # v = v / np.linalg.norm(v, axis=0, ord=2)\n",
        "        vecs.append(v)\n",
        "    return vecs\n",
        "\n",
        "def get_sentence_vector(sentence):\n",
        "    vecs = get_token_vectors(sentence)\n",
        "    if len(vecs) == 0:\n",
        "        return None\n",
        "    else:\n",
        "        # return np.array(vecs).max(axis=0)\n",
        "        # return np.array(vecs).mean(axis=0)\n",
        "        return np.array(vecs).sum(axis=0)\n",
        "        # return np.concatenate([np.array(vecs).max(axis=0), np.array(vecs).mean(axis=0)])\n",
        "\n",
        "def is_stop(token):\n",
        "    return (token.pos in stop_pos \n",
        "            or token.base in stop_word \n",
        "            or any([r for r in stop_word_regex if r.match(token.base) is not None]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCuGmYnRhsTb"
      },
      "source": [
        "get_sentence_vector(\"与えられた文から文の分散表現を計算します。\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxi0CKEwhudM"
      },
      "source": [
        "from tqdm import tqdm\n",
        "for item in tqdm(items):\n",
        "    desc = item[\"desc\"]\n",
        "    desc_vec = get_sentence_vector(desc)\n",
        "    item[\"vec\"] = desc_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DERnyNfUhzuw"
      },
      "source": [
        "from IPython.display import display, HTML, clear_output\n",
        "from html import escape\n",
        "\n",
        "def search_irasuto(sentence, top_n=3):\n",
        "    sentence_vector = get_sentence_vector(sentence)\n",
        "    sims = []\n",
        "    if sentence_vector is None:\n",
        "        print(\"検索できない文章です。もう少し文章を長くしてみてください。\")\n",
        "    else:\n",
        "        for item in items:\n",
        "            v = item[\"vec\"]\n",
        "            if v is None:\n",
        "                sims.append(1.0)\n",
        "            else:\n",
        "                sim = sentence_similarity(sentence_vector, v)\n",
        "                sims.append(sim)\n",
        "    \n",
        "    count = 0\n",
        "    for index in np.argsort(sims):\n",
        "        if count >= top_n:\n",
        "            break\n",
        "        item = items[index]\n",
        "        desc = escape(item[\"desc\"])\n",
        "        imgs = item[\"imgs\"]\n",
        "        if len(imgs) == 0:\n",
        "            continue\n",
        "        img = imgs[0]\n",
        "        page = item[\"page\"]\n",
        "        sim = sims[index]\n",
        "        display(HTML(\"<div><a href='\" + page + \"' target='_blank' rel='noopener noreferrer'><img src='\" + img + \"' width='100'>\" + str(sim) + \": \" + desc + \"</a><div>\"))\n",
        "        count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKIhqhnBh3U6"
      },
      "source": [
        "search_irasuto(sentence=\"暴走したAI\", top_n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CI-7oNwlh-zK"
      },
      "source": [
        "# 青空文庫 北大路魯山人「だしの取り方」\n",
        "# https://www.aozora.gr.jp/cards/001403/files/49986_37674.html\n",
        "document_text = \"\"\"\n",
        "かつおぶしはどういうふうに選択し、どういうふうにして削るか。まず、かつおぶしの良否の簡単な選択法をご披露しよう。よいかつおぶしは、かつおぶしとかつおぶしとを叩き合わすと、カンカンといってまるで拍子木か、ある種の石を鳴らすみたいな音がするもの。虫の入った木のように、ポトポトと音のする湿っぽい匂いのするものは悪いかつおぶし。\n",
        "本節と亀節ならば、亀節がよい。見た目に小さくとも、刺身にして美味い大きいものがやはりかつおぶしにしても美味だ。見たところ、堂々としていても、本節は大味で、値も亀節の方が安く手に入る。\n",
        "次に削り方だが、まず切れ味のよい鉋を持つこと。切れ味の悪い鉋ではかつおぶしを削ることはむずかしい。赤錆になったり刃の鈍くなったもので、ゴリゴリとごつく削っていたのでは、かつおぶしがたとえ百円のものでも、五十円の値打ちすらないものになる。\n",
        "どんなふうに削ったのがいいだしになるかというと、削ったかつおぶしがまるで雁皮紙のごとく薄く、ガラスのように光沢のあるものでなければならない。こういうのでないと、よいだしが出ない。削り下手なかつおぶしは、死んだだしが出る。生きたいいだしを作るには、どうしても上等のよく切れる鉋を持たねばならない。そしてだしをとる時は、グラグラッと湯のたぎるところへ、サッと入れた瞬間、充分にだしができている。それをいつまでも入れておいて、クタクタ煮るのではろくなだしは出ず、かえって味をそこなうばかりである。いわゆる二番だしというようなものにしてはいけない。\n",
        "そこで、まず第一に、刃の切れる、台の平らな鉋をお持ちになることをお勧めしたい。かつおぶしを非常に薄く削るということは経済的であり、能率的でもある。\n",
        "なお、わたしの案ずるところでは、百の家庭のうち九十九までがいい鉋を持っていまい。料理を講義する人でも、持っていないのだから、一般家庭によい鉋を持っている家は一応ないと考えて差し支えない。\n",
        "さて鉋はいつでも切れるようにしておかなければならない。しかし、素人ではよく研げないから、大工とか仕事をするひとに研いでもらえばいい。そのほか、とぎや専門という商売もあるのだから、いつも大工の鉋のようによく切れるようにしておかなければ、料理をしようとする時にまごつくのがオチだ。\n",
        "日本にはかつおぶしがたくさんあるので、そう重きをおいていないが、外国にあったら大変なことだ。外国人はかつおを知らないし、従ってかつおぶしを知らない。牛乳とか、バターとか、チーズのようなもの一本で料理をしている。しかし、これは不自由なことであって、かつおぶしのある日本人はまことに幸せである。ゆえに、かつおぶしを使って美味料理の能率をあげることを心がけるのがよい。味、栄養もいいし、よい材料を選べば、世界に類のないよいスープができる。\n",
        "それなのに、かつおぶしに対する知識もなく、削り方も、削って使う方法も知らないのは、情けないことだ。その上削る道具もない――これはものの間違いで、大いに反省してもらいたいことだ。現在、鉋でかつおぶしを削っているのは料理屋のみであって、たいがいは道具もなくて我慢しているようである。その料理屋さえ最近削りかつおぶしを使用している。削り節にもいろいろあって、最上の削り節ならば、まずまずであるが、削り節は削り立てがいいので、時がたってはよろしくない。\n",
        "鉋があっても、切れない場合が多いし、それを使用して削れないと思うくらいなら、日本料理をやめた方がいい。\n",
        "料理にかぎらず、やるというのなら、どんなことでもやるのが当然で、やらなければ達成できない。かといって、この場合、料理屋の真似をしてガラスで削るのは危険だし、たくさん削る場合は間に合わないから、無理をしてかつおぶしを削ることになる。しかし、無理をすることは味が死ぬことになるのであるから、生きた味を出すためには、よく切れる鉋にかぎるのである。\n",
        "鉋を持ってないひとがいたら、ここで一奮起して、大工の使用している鉋を購入するようお勧めしたい。大工の鉋一つ買うことは、値段からいっても高価ではないし、生涯なくなるものでもないのだから、不経済にはならない。要は研げないと頭からきめてかからずに、インチキ鉋の使用を一刻も早くやめる必要があろう。\n",
        "さて昆布だしのことは、東京では一流の料理屋以外はあまり知らないようだ。これは、東京には昆布を使うという習慣が昔からなかったからだろう。昆布のだしは実に結構なものであって、魚の料理には昆布だしにかぎる。かつおぶしのだしでは魚の味が二つ重なるので、どうしても具合の悪いものが出来る。味のダブルということはくどいのである。昆布をだしに使う方法は、古来京都で考えられた。周知のごとく、京都は千年も続いた都であったから、実際上の必要に迫られて、北海道で産出される昆布を、はるかな京都という山の中で、昆布だしを取るまでに発達させたのである。\n",
        "昆布のだしを取るには、まず昆布を水でぬらしただけで一、二分ほど間をおき、表面がほとびた感じが出た時、水道の水でジャーッとやらずに、トロトロと出るくらいに昆布に受けながら、指先で器用にいたわって、だましだまし表面の砂やゴミを落とし、その昆布を熱湯の中へサッと通す。それでいいのだ。これではだしが出たかどうか、心配なさるかも知れない。出たか出ないかはちょっと汁を吸ってみれば、無色透明でも、うま味が出ているのがわかる。量はどのくらい入れるかは実習すれば、すぐにわかる。このだしはなどの時はぜひなくてはならない。\n",
        "こぶを湯にさっと通したきりで上げてしまうのは、なにか惜しいように考え、長くいつまでも煮るのは愚の骨頂、昆布の底の甘味が出て、決して気の利いただしはできない。京都辺では引出し昆布といって、鍋の一方から長い昆布を入れ、底をくぐらして一方から引き上げるというやり方もあるが、こういうきびしいやり方だと、どんなやかましい食通たちでも、文句のいいようがないということになっている。\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ3k2STr234S"
      },
      "source": [
        "import functools\n",
        "\n",
        "from ja_sentence_segmenter.common.pipeline import make_pipeline\n",
        "from ja_sentence_segmenter.concatenate.simple_concatenator import concatenate_matching\n",
        "from ja_sentence_segmenter.split.simple_splitter import split_newline, split_punctuation\n",
        "\n",
        "def split_sentences(text):\n",
        "    split_punc2 = functools.partial(split_punctuation, punctuations=r\"。!?\")\n",
        "    concat_tail_te = functools.partial(concatenate_matching, former_matching_rule=r\"^(?P<result>.+)(て)$\", remove_former_matched=False)\n",
        "    segmenter = make_pipeline(normalize_text, split_newline, concat_tail_te, split_punc2)\n",
        "    return segmenter(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h82tDkhCcBj3"
      },
      "source": [
        "sentences = list(split_sentences(document_text.replace(\"\\r\\n\", \"\\n\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6mwDTBa3EEL"
      },
      "source": [
        "import termextract.mecab\n",
        "import termextract.core\n",
        "\n",
        "mecab_text = \"\"\n",
        "for sentence in sentences:\n",
        "    tokens = tokenize(sentence)\n",
        "    if tokens:\n",
        "        mecab_text += \"\".join([token.line + \"\\n\" for token in tokens]) + \"EOS\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbWVQNw13NDa"
      },
      "source": [
        "import collections\n",
        "\n",
        "term_frequency = termextract.mecab.cmp_noun_dict(mecab_text)\n",
        "term_lr = termextract.core.score_lr(term_frequency, ignore_words=termextract.mecab.IGNORE_WORDS, lr_mode=1, average_rate=1)\n",
        "term_importance = termextract.core.term_importance(term_frequency, term_lr)\n",
        "term_collection = collections.Counter(term_importance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCMnB8sr3PRw"
      },
      "source": [
        "irasuto_queries = []\n",
        "\n",
        "QUERY_LIMIT = 5\n",
        "\n",
        "for tokenized_term, score in term_collection.most_common()[:QUERY_LIMIT]:\n",
        "    query = termextract.core.modify_agglutinative_lang(tokenized_term)\n",
        "    irasuto_queries.append(query)\n",
        "    print(query + \" \" + str(score))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPEPG5FY44U9"
      },
      "source": [
        "from IPython.display import display, HTML, clear_output\n",
        "from html import escape\n",
        "\n",
        "def to_irasuto_dom(item, similarity):\n",
        "    imgs = item[\"imgs\"]\n",
        "    if not imgs:\n",
        "        return None\n",
        "    desc = escape(item[\"desc\"])\n",
        "    page = item[\"page\"]\n",
        "    dom_source = \"<div>\" + \"\".join([\"<img src='\" + img + \"' width='100' onclick='window.open(\\\"\" + page + \"\\\", \\\"_blank\\\", \\\"noopener,noreferrer\\\");\" + \"\"\"\n",
        "        var thisImage = this.parentNode;\n",
        "        thisImage.parentNode.setAttribute(\"style\", \"padding:10px 10px 10px 10px;\");\n",
        "        var unusedImages = [];\n",
        "        for (var node of thisImage.parentNode.childNodes) {\n",
        "            if (node !== thisImage) {\n",
        "                unusedImages.push(node);\n",
        "            } else {\n",
        "                for (var siblingNode of node.childNodes) {\n",
        "                    if (siblingNode !== this) {\n",
        "                        unusedImages.push(siblingNode);\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            for (var imgNode of node.getElementsByTagName(\"img\")) {\n",
        "                imgNode.removeAttribute(\"onclick\");\n",
        "                /* imgNode.setAttribute(\"style\", Math.random() >= 0.5? \"float:left\" : \"float:right\"); */\n",
        "            }\n",
        "        }; \n",
        "        for (var node of unusedImages) {\n",
        "            node.remove();\n",
        "        }'>\"\"\" for img in imgs]) + \"<span>\" + str(similarity) + \": \" + desc + \"</span></div>\"\n",
        "    return dom_source\n",
        "\n",
        "def to_irasuto_recommendation_dom(query, top_n=3):\n",
        "    query_vector = get_sentence_vector(query)\n",
        "    sims = []\n",
        "    if query_vector is None:\n",
        "        #print(\"検索できない文章です。もう少し文章を長くしてみてください。\")\n",
        "        return None\n",
        "    else:\n",
        "        for item in items:\n",
        "            v = item[\"vec\"]\n",
        "            if v is None:\n",
        "                sims.append(1.0)\n",
        "            else:\n",
        "                sim = sentence_similarity(query_vector, v)\n",
        "                sims.append(sim)\n",
        "    \n",
        "    count = 0\n",
        "    irasuto_doms = []\n",
        "    for index in np.argsort(sims):\n",
        "        if count >= top_n:\n",
        "            break\n",
        "        irasuto_dom = to_irasuto_dom(items[index], sims[index])\n",
        "        if irasuto_dom is not None:\n",
        "            irasuto_doms.append(irasuto_dom)\n",
        "            count += 1\n",
        "    \n",
        "    return (\"<div style='background:rgb(253,243,243); padding:10px 10px 10px 10px;'><h3>キーワード「\" \n",
        "            + escape(query) + \"」に合いそうな画像を探してみました。気に入ったものをクリックしてください。</h3>\" \n",
        "            + \"\".join(irasuto_doms) + \"<button onclick='this.parentNode.remove();'>この中にはない</button></div>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TZQdtOC6Yvp"
      },
      "source": [
        "queries = irasuto_queries\n",
        "document_html = \"\"\n",
        "for sentence in sentences:\n",
        "    document_html += \"<span>\" + escape(sentence) + \"</span><br>\"\n",
        "    for query in queries:\n",
        "        if query in sentence:\n",
        "            document_html += to_irasuto_recommendation_dom(query=query, top_n=10)\n",
        "    queries = [query for query in queries if query not in sentence]\n",
        "document_html += \"\"\n",
        "display(HTML(document_html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_YeSrnah5k6"
      },
      "source": [
        "search_irasuto(sentence=\"いらすとやさんに惜しみない拍手を\", top_n=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Rovv_gDh8oT"
      },
      "source": [
        "search_irasuto(sentence=\"つづく\", top_n=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWS76eQWcCFu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}